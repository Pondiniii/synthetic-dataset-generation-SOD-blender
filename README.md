# Synthetic SOD Dataset Generator

Automated pipeline for generating synthetic Salient Object Detection (SOD) training datasets using BlenderKit free assets and GPU-accelerated rendering.

## üöÄ Quick Start

**New here?** ‚Üí Read **[docs/QUICK_START.md](docs/QUICK_START.md)** for setup ‚Üí download ‚Üí render in 15 minutes!

**Full documentation**: **[docs/INDEX.md](docs/INDEX.md)**

## ‚ú® Features

- **BlenderKit Integration**: Scrape and download 100,000+ free 3D models + HDR environments
- **GPU Rendering**: CUDA/OptiX accelerated (Blender Cycles)
- **Random Augmentations**: Angles, HDR rotation, lighting, camera distance (no grid bias)
- **JPEG XL Output**: Lossless compression (~40% smaller than PNG)
- **High-Resolution**: 640px to 4096px support with OptiX denoising
- **Perfect Binary Masks**: 0-255 alpha channel extraction with anti-aliasing
- **Realistic Lighting**: 2K/4K .exr HDRI backgrounds with reflections
- **Production Ready**: 0.6s-12s per image, multiprocessing support (12x speedup)
- **Scene Reuse**: Load model once, render 40 views (30-40% faster)

## Output Structure

```
datasets/
‚îî‚îÄ‚îÄ {dataset_name}/          # Auto-generated: {model_id}_{hdr_id}
    ‚îú‚îÄ‚îÄ images/              # RGB images with HDR backgrounds
    ‚îÇ   ‚îú‚îÄ‚îÄ view_000_00.png
    ‚îÇ   ‚îú‚îÄ‚îÄ view_000_11.png
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ masks/               # Binary segmentation masks (0-255 with anti-aliasing)
        ‚îú‚îÄ‚îÄ view_000_00.png
        ‚îú‚îÄ‚îÄ view_000_11.png
        ‚îî‚îÄ‚îÄ ...
```

**Filename format**: `view_{yaw:03d}_{pitch:02d}.png`
- `yaw`: 0-359¬∞ (horizontal rotation around object)
- `pitch`: 0-90¬∞ (elevation angle, 0=horizontal, 90=top-down)
- **Example**: `view_000_00.png` = front view at horizon level

**Dataset naming**:
- Auto-generated: First 8 chars of model + HDR UUIDs (e.g., `718316ee_14d3a10a`)
- Custom: Pass `dataset_name` parameter to override

## Quick Start

**English**:
```bash
# 1. Setup scraper
cd blenderkit-scraper
echo 'BLENDERKIT_API_KEY=your_key_here' > .env
cargo build --release

# 2. Download 5 models + 2 HDRs
cargo run -r -- scrape --asset-type model --limit 5 --output models.json
cargo run -r -- scrape --asset-type hdr --limit 2 --output hdrs.json
cargo run -r -- download --input models.json --output downloads --yes
cargo run -r -- download --input hdrs.json --output downloads --yes

# 3. Render dataset (5 models √ó 2 HDRs √ó 96 views = 960 images)
cd ..
for model in blenderkit-scraper/downloads/model/*.blend; do
    for hdr in blenderkit-scraper/downloads/hdr/*.exr; do
        blender --background --python blender_renderer/render_dataset.py -- \
            "$model" "$hdr" datasets/ 12 8
    done
done
```

**Polski**:
```bash
# 1. Konfiguracja scrapera
cd blenderkit-scraper
echo 'BLENDERKIT_API_KEY=twoj_klucz_api' > .env
cargo build --release

# 2. Pobierz 5 modeli + 2 HDR
cargo run -r -- scrape --asset-type model --limit 5 --output models.json
cargo run -r -- scrape --asset-type hdr --limit 2 --output hdrs.json
cargo run -r -- download --input models.json --output downloads --yes
cargo run -r -- download --input hdrs.json --output downloads --yes

# 3. Renderuj dataset (5 modeli √ó 2 HDR √ó 96 widok√≥w = 960 zdjƒôƒá)
cd ..
for model in blenderkit-scraper/downloads/model/*.blend; do
    for hdr in blenderkit-scraper/downloads/hdr/*.exr; do
        blender --background --python blender_renderer/render_dataset.py -- \
            "$model" "$hdr" datasets/ 12 8
    done
done
```

**Result**: `datasets/*/images/*.png` + `datasets/*/masks/*.png` ready for training!

## Requirements

- **Blender 3.x+** with GPU support (CUDA/OptiX)
- **Rust 1.70+** (for BlenderKit scraper)
- **Python 3.9+** with PIL/numpy (bundled in Blender)
- **BlenderKit API Key**: Get from [blenderkit.com](https://www.blenderkit.com/api_keys/)
- **GPU**: NVIDIA with 4GB+ VRAM recommended

## Installation

### 1. Setup BlenderKit Scraper

```bash
cd blenderkit-scraper
cargo build --release

# Configure API key
echo 'BLENDERKIT_API_KEY=your_key_here' > .env
```

### 2. Verify Blender Python

```bash
blender --background --python-expr "import bpy; print(f'Blender {bpy.app.version_string}')"
```

## Usage

### Step 1: Scrape BlenderKit Assets

Search and save asset metadata:

```bash
cd blenderkit-scraper
cargo run --release -- search --query "furniture" --asset-type model --limit 50
cargo run --release -- search --query "outdoor" --asset-type hdr --limit 20
```

**Output**: `search_results_{model|hdr}_{timestamp}.json`

### Step 2: Download Assets

Download .blend models and .exr HDRs:

```bash
# Download all scraped models
cargo run --release -- download --input search_results_model_*.json --output downloads/model

# Download all scraped HDRs (2K .exr format)
cargo run --release -- download --input search_results_hdr_*.json --output downloads/hdr
```

**Critical**: HDRs download as `resolution_2K` (.exr, ~10MB, 32-bit float), NOT JPEG thumbnails.

### Step 3: Render Dataset

#### Option A: Single Render

```bash
blender --background --python blender_renderer/render_dataset.py -- \
    downloads/model/your_model.blend \
    downloads/hdr/your_hdr.exr \
    datasets/ \
    12 \
    8
```

#### Option B: Batch Render (Multiprocessing - RECOMMENDED)

```bash
# Render all combinations with 4 parallel workers
python3 batch_render.py \
    --models blenderkit-scraper/downloads/model/*.blend \
    --hdrs blenderkit-scraper/downloads/hdr/*.exr \
    --output datasets \
    --workers 4 \
    --yaw 12 --pitch 8

# Use all CPU cores (auto-detect)
python3 batch_render.py \
    --models downloads/model/*.blend \
    --hdrs downloads/hdr/*.exr \
    --output datasets \
    --workers auto
```

**Parameters**:
- `model.blend` - Path to 3D model
- `hdr.exr` - Path to HDR environment
- `datasets/` - Output directory
- `--workers` - Parallel Blender instances (1-16 or "auto")
- `--yaw 12` - Yaw steps (360¬∞/12 = 30¬∞ increments)
- `--pitch 8` - Pitch steps (90¬∞/8 = ~11¬∞ increments)

**Performance**:
- 1 worker: ~0.5s per image (sequential)
- 4 workers: ~4x speedup (parallel)
- Auto workers: Uses all CPU cores

**Output**: Creates `datasets/{model_id}_{hdr_id}/` for each combination

#### Option C: Augmented Renderer (High-Res + JPEG XL + Random Augmentations) - RECOMMENDED

For production datasets with high-resolution images, JPEG XL lossless compression, and built-in augmentations:

```bash
# High-res augmented rendering (2048px, JPEG XL, 40 views)
python3 batch_render.py \
    --models $(ls blenderkit-scraper/downloads/model/*.blend) \
    --hdrs $(ls blenderkit-scraper/downloads/hdr/*.exr) \
    --output datasets_highres \
    --workers 4 \
    --resolution 2048 \
    --pitch 10 --yaw-per-pitch 4 \
    --format jxl \
    --augmented

# Ultra-high-res (4096px) for research
python3 batch_render.py \
    --models downloads/model/*.blend \
    --hdrs downloads/hdr/*.exr \
    --output datasets_4k \
    --workers 2 \
    --resolution 4096 \
    --pitch 10 --yaw-per-pitch 4 \
    --format jxl \
    --augmented
```

**Parameters**:
- `--resolution` - Output resolution: 640 / 1024 / 2048 / 4096 (default: 640)
- `--pitch` - Number of pitch angles 0-90¬∞ (default: 10)
- `--yaw-per-pitch` - Random yaw angles per pitch (default: 4)
- `--hdr-switch` - Switch HDR every N images (default: 4)
- `--format` - Output format: png / jxl (default: jxl)
- `--augmented` - Enable augmentations (auto-enabled for high-res or JXL)

**Built-in Augmentations**:
- **Random Angles**: No grid pattern bias (10 pitch √ó 4 random yaw = 40 views)
- **HDR Rotation**: Random 0-360¬∞ rotation per image
- **HDR Switching**: Different HDR every 4 images (maximum lighting diversity)
- **Lighting Variation**: Random strength 0.7-1.5x per image
- **Camera Distance**: Tight (0.85x) / Medium (1.0x) / Loose (1.3x) framing
- **JPEG XL**: Lossless compression (~40% smaller than PNG, ~60% smaller than RGBA)
- **OptiX Denoising**: Auto-enabled for resolution ‚â•2048
- **Scene Reuse**: Load model once, render all views (30-40% faster)

**Output Structure** (Flat UUID naming):
```
datasets_highres/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ {session}_{model}_{hdr}_{yaw}_{pitch}_{dist}_{rot}_{str}.jxl
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ masks/
    ‚îú‚îÄ‚îÄ {session}_{model}_{hdr}_{yaw}_{pitch}_{dist}_{rot}_{str}.jxl
    ‚îî‚îÄ‚îÄ ...
```

**Filename encoding**:
- `session` - 8-char session UUID (shared across batch)
- `model` - 8-char model UUID
- `hdr` - 8-char HDR UUID
- `yaw` - Yaw angle in degrees (000-359)
- `pitch` - Pitch angle in degrees (00-90)
- `dist` - Camera distance: `t` (tight) / `m` (medium) / `l` (loose)
- `rot` - HDR rotation in degrees (000-359)
- `str` - Light strength (070-150 = 0.7x-1.5x)

**Example**: `a1b2c3d4_e5f6g7h8_i9j0k1l2_045_30_m_180_120.jxl`
- Session: a1b2c3d4
- Model: e5f6g7h8
- HDR: i9j0k1l2
- Yaw: 45¬∞, Pitch: 30¬∞
- Medium distance, HDR rotated 180¬∞, light strength 1.2x

**Performance** (RTX 3080, 24 cores):
| Resolution | Samples | Denoising | Time/Image | Workers | Total Time (20 models √ó 2 HDRs √ó 40 views) |
|------------|---------|-----------|------------|---------|---------------------------------------------|
| 640x640    | 64      | No        | ~0.6s      | 12      | ~4 min                                      |
| 1024x1024  | 64      | No        | ~1.2s      | 12      | ~8 min                                      |
| 2048x2048  | 128     | Yes       | ~3.5s      | 8       | ~18 min                                     |
| 4096x4096  | 128     | Yes       | ~12s       | 4       | ~80 min                                     |

**File Sizes** (per image):
- PNG RGB: ~350KB (640px), ~1.2MB (2048px), ~4.5MB (4096px)
- JPEG XL RGB: ~210KB (640px), ~700KB (2048px), ~2.7MB (4096px) **‚Üê 40% smaller**
- PNG Mask: ~50KB (640px), ~180KB (2048px), ~650KB (4096px)
- JPEG XL Mask: ~30KB (640px), ~110KB (2048px), ~390KB (4096px) **‚Üê 40% smaller**

**Storage Savings**:
- 20 models √ó 2 HDRs √ó 40 views = 1,600 image pairs
- PNG: ~640MB (640px), ~2.2GB (2048px), ~8.2GB (4096px)
- JPEG XL: ~380MB (640px), ~1.3GB (2048px), ~5.0GB (4096px) **‚Üê Save 3GB at 4K!**

**Use Cases**:
- **High-res baseline**: Render 2048px or 4096px, apply random crops during training
- **Multi-scale robustness**: Train on tight/medium/loose crops simultaneously
- **Lighting generalization**: Random HDR rotation + switching + intensity
- **Storage optimization**: JPEG XL lossless = PNG quality at 60% size
- **No grid bias**: Random angles better for AI than regular grid patterns

## Technical Details

### Mask Generation (0-255 Binary)

**Method**: RGBA alpha channel extraction (NOT BW rendering mode)

```python
# 1. Render RGB with HDR background
scene.render.film_transparent = False
scene.render.image_settings.color_mode = 'RGB'
bpy.ops.render.render(write_still=True)

# 2. Render RGBA with transparent background
scene.render.film_transparent = True
scene.render.image_settings.color_mode = 'RGBA'
bpy.ops.render.render(write_still=True)

# 3. Extract alpha channel ‚Üí 0-255 grayscale
alpha = np.array(Image.open(rgba_path).split()[3])
Image.fromarray(alpha, mode='L').save(mask_path)
```

**Result**: Grayscale masks with smooth anti-aliasing:
- **0** = background
- **255** = object interior
- **1-254** = edge pixels (~0.7% of mask, smooth anti-aliased boundaries)

This is **optimal for SOD training** - neural networks (U-2-Net, TRACER) learn better from soft boundaries than hard binary masks.

### HDR vs JPEG Backgrounds

| Format | Bit Depth | Lighting | Reflections | SOD Quality |
|--------|-----------|----------|-------------|-------------|
| .exr HDR | 32-bit float | Realistic | Yes | High |
| JPEG | 8-bit | Flat | No | Poor |

**Always use .exr HDRs** for production datasets.

### Spherical Camera Math

```python
# Convert (yaw, pitch, distance) ‚Üí (x, y, z)
x = distance * cos(pitch) * sin(yaw)
y = distance * cos(pitch) * cos(yaw)
z = distance * sin(pitch)

# Point camera at origin
camera.location = (x, y, z)
direction = Vector((0,0,0)) - camera.location
camera.rotation_euler = direction.to_track_quat('-Z', 'Y').to_euler()
```

### GPU Optimization

```python
# Reduce VRAM usage (2GB ‚Üí 1GB)
scene.cycles.samples = 64              # Lower = faster
scene.cycles.use_denoising = False     # Save ~2GB VRAM
scene.cycles.tile_size = 256           # GPU optimal
```

**VRAM Usage**: ~970MB peak per render (tested RTX 3080).

## Troubleshooting

### "System out of GPU memory"

1. Close VRAM-hungry apps (LM Studio, games)
2. Reduce samples: `scene.cycles.samples = 32`
3. Disable OptiX denoiser (already done in code)
4. Use smaller resolution: `BlenderRenderer(output_res=512)`

### Masks have intermediate values (not just 0 and 255)

This is **intentional and correct**! Anti-aliased edges (values 1-254) are optimal for SOD training.

**Verify mask quality**:
```python
from PIL import Image
import numpy as np

mask = np.array(Image.open('datasets/test/masks/view_000_00.png'))
unique, counts = np.unique(mask, return_counts=True)
print(f"Range: {mask.min()}-{mask.max()}")  # Should be 0-255
print(f"Unique values: {len(unique)}")      # ~60 values (anti-aliasing gradient)
print(f"Edge pixels: {len(mask[(mask>0) & (mask<255)])} ({len(mask[(mask>0) & (mask<255)])/mask.size*100:.2f}%)")
```

**Expected output**: ~0.7% edge pixels with smooth gradient (optimal for neural networks).

### Downloaded HDRs are JPEG (not .exr)

**Cause**: Scraper downloading `thumbnail` instead of `resolution_2K`.

**Fix**: Update `blenderkit-scraper/src/main.rs`:
```rust
let (download_url, file_ext) = if asset.asset_type == "hdr" {
    // Download resolution_2K (.exr)
    let url = asset.files.iter()
        .find(|f| f.file_type == "resolution_2K")
        .and_then(|f| f.download_url.as_ref());
    (url, "exr")
} else {
    // Download blend file
    (blend_url, "blend")
};
```

### Object too small/large in frame

**Fix**: Adjust camera distance in `render_dataset()`:
```python
camera = self.create_camera(distance=3.0)  # Increase for zoom out
```

Or modify normalization scale:
```python
scale_factor = 0.8 / max_dim  # Make object smaller
```

## Example Workflows

### Quick Test (1 model √ó 1 HDR √ó 12 views)

```bash
# Download one model and HDR
cd blenderkit-scraper
cargo run -r -- scrape --asset-type model --limit 1 --output models.json
cargo run -r -- scrape --asset-type hdr --limit 1 --output hdrs.json
cargo run -r -- download --input models.json --output downloads --yes
cargo run -r -- download --input hdrs.json --output downloads --yes

# Render 12 views
cd ..
blender --background --python blender_renderer/render_dataset.py -- \
    blenderkit-scraper/downloads/model/*.blend \
    blenderkit-scraper/downloads/hdr/*.exr \
    datasets/ \
    12 1
```

**Result**: `datasets/{model_id}_{hdr_id}/` with 12 image pairs

### Production Dataset (100 models √ó 10 HDRs √ó 96 views = 96K images)

```bash
# 1. Scrape assets
cd blenderkit-scraper
cargo run -r -- scrape --asset-type model --limit 100 --output models.json
cargo run -r -- scrape --asset-type hdr --limit 10 --output hdrs.json

# 2. Download (with size limit)
cargo run -r -- download --input models.json --output downloads --max-size 5000 --yes
cargo run -r -- download --input hdrs.json --output downloads --yes

# 3. Render all combinations
cd ..
for model in blenderkit-scraper/downloads/model/*.blend; do
    for hdr in blenderkit-scraper/downloads/hdr/*.exr; do
        blender --background --python blender_renderer/render_dataset.py -- \
            "$model" "$hdr" datasets/ 12 8
    done
done
```

**Result**: `datasets/*/{images,masks}/*.png` ready for SOD training

## Dataset Statistics

Production tested with **8000+ images**:
- **U-2-Net**: Excellent salient object detection results
- **TRACER**: High-quality segmentation on complex scenes
- **Render Speed**: ~0.5s per image pair (RTX 3080, 64 samples, 640x640)
- **Mask Quality**: 0-255 grayscale with ~0.7% smooth anti-aliased edges
- **Storage**: ~200KB per image pair (PNG compressed)

## Why This Pipeline?

**vs blenderproc2**:
- ‚úÖ GPU acceleration (10x faster rendering)
- ‚úÖ Proper anti-aliased masks (alpha extraction, not BW mode)
- ‚úÖ Stable Blender API (native bpy, no wrapper bugs)
- ‚úÖ Production tested (8K+ images for U-2-Net/TRACER)

**vs Manual Blender**:
- ‚úÖ Automated asset scraping (BlenderKit API via Rust CLI)
- ‚úÖ Batch rendering with spherical camera coverage
- ‚úÖ Consistent dataset structure and naming
- ‚úÖ Reproducible and scriptable

## Project Structure

```
synthetic-dataset-generation-SOD-blender/
‚îú‚îÄ‚îÄ blenderkit-scraper/          # Rust CLI for BlenderKit API
‚îÇ   ‚îú‚îÄ‚îÄ src/main.rs              # API client, search, download
‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml               # Rust dependencies
‚îÇ   ‚îú‚îÄ‚îÄ .env                     # API key (BLENDERKIT_API_KEY=...)
‚îÇ   ‚îî‚îÄ‚îÄ downloads/               # Downloaded assets
‚îÇ       ‚îú‚îÄ‚îÄ model/*.blend        # 3D models (UUID.blend)
‚îÇ       ‚îî‚îÄ‚îÄ hdr/*.exr            # HDR environments (UUID.exr)
‚îú‚îÄ‚îÄ blender_renderer/
‚îÇ   ‚îî‚îÄ‚îÄ render_dataset.py        # Headless Blender renderer
‚îú‚îÄ‚îÄ datasets/                    # Generated SOD datasets
‚îÇ   ‚îî‚îÄ‚îÄ {model_id}_{hdr_id}/
‚îÇ       ‚îú‚îÄ‚îÄ images/*.png         # RGB images with HDR backgrounds
‚îÇ       ‚îî‚îÄ‚îÄ masks/*.png          # Grayscale segmentation masks
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îî‚îÄ‚îÄ README.md                    # This file
```

## License

Code: MIT
BlenderKit Assets: Check individual asset licenses on blenderkit.com

## Credits

- **BlenderKit**: Free 3D asset repository
- **Blender Foundation**: Cycles renderer
- **U-2-Net/TRACER**: SOD neural networks tested with this pipeline

---

**Author**: Generated 8K+ synthetic SOD images for ML training
**Status**: Production ready (tested with RTX 3080, Blender 3.x)
**Performance**: ~0.5s per render, 96 views in ~48 seconds
